{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report - Lab: SO Perceptron\n",
    "\n",
    "name: Filip Špidla\n",
    "\n",
    "email: spidlfil@fel.cvut.cz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# load single example\n",
    "def load_example( img_path ):\n",
    "\n",
    "    Y = img_path[img_path.rfind('_')+1:-4]\n",
    "\n",
    "    img = Image.open( img_path )\n",
    "    img_mat = np.asarray( img )\n",
    "    \n",
    "    n_letters = len( Y )\n",
    "    im_height = int(img_mat.shape[0])\n",
    "    im_width = int(img_mat.shape[1]/n_letters)\n",
    "    n_pixels = im_height*im_width\n",
    "    \n",
    "    X = np.zeros( [int(n_pixels+n_pixels*(n_pixels-1)/2),n_letters])\n",
    "    for i in range(n_letters):\n",
    "        \n",
    "        # single letter\n",
    "        letter = img_mat[:,i*im_width:(i+1)*im_width]/255\n",
    "        \n",
    "        # compute features\n",
    "        x = letter.flatten()\n",
    "        X[0:len(x),i] = x\n",
    "        cnt = n_pixels \n",
    "        for j in range(0,n_pixels-1):\n",
    "            for k in range(j+1,n_pixels):\n",
    "                X[cnt,i] = x[j]*x[k]\n",
    "                cnt = cnt + 1\n",
    "           \n",
    "        X[:,i] = X[:,i]/np.linalg.norm(X[:,i])\n",
    "        \n",
    "    return X, Y, img\n",
    "    \n",
    "# load all examples from a folder    \n",
    "def load_examples( image_folder ):\n",
    "    \n",
    "    files = [f for f in listdir(image_folder) if isfile(join(image_folder, f))]\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    img = []\n",
    "    for file in listdir(image_folder):\n",
    "        path = join(image_folder, file)\n",
    "        if isfile( path ):\n",
    "                        \n",
    "            X_,Y_,img_ = load_example( path )\n",
    "            X.append( X_ )\n",
    "            Y.append( Y_ )\n",
    "            img.append( img_ )\n",
    "        \n",
    "    return X, Y, img\n",
    "    \n",
    "# load training examples\n",
    "trn_X, trn_Y, trn_img = load_examples( 'ocr_names_images/trn' )\n",
    "\n",
    "# load testing examples\n",
    "tst_X, tst_Y, tst_img = load_examples( 'ocr_names_images/tst' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1 (3 points) Implement the Perceptron algorithm for learning parameters (w ∈\n",
    "R\n",
    "d·|A|\n",
    ", b ∈ R\n",
    "|A|) of the linear multi-class classifier (1). Use the provided training examples T\n",
    "m\n",
    "to learn parameters of the classifier. Report the sequence prediction error Rseq and the character\n",
    "prediction error Rchar computed on the provided testing examples S\n",
    "l\n",
    ". The output should be a\n",
    "single script (Jupyter notebook or Matlab) which learns the classifier and prints the computed\n",
    "testing errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Prediction Error (R_seq): 0.13\n",
      "Character Prediction Error (R_char): 0.11567516525023608\n"
     ]
    }
   ],
   "source": [
    "def perceptron_train(X, Y, epochs=100):\n",
    "\n",
    "    #Map each unique label in Y to a unique integer identifier\n",
    "    unique_labels = list(set(Y))\n",
    "    label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    num_features = X[0].shape[0]  \n",
    "    num_classes = len(unique_labels)  #\n",
    "\n",
    "    weights = np.zeros((num_features, num_classes))\n",
    "    biases = np.zeros(num_classes)\n",
    "\n",
    "    #training loop ofor each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for features, label in zip(X, Y):\n",
    "            label_id = label_to_id[label]\n",
    "\n",
    "            summed_features = np.sum(features, axis=1)\n",
    "            scores = np.dot(summed_features, weights) + biases\n",
    "            predicted_class = np.argmax(scores)\n",
    "\n",
    "            #update weights and biases if the prediction is incorrect\n",
    "            if predicted_class != label_id:\n",
    "                weights[:, label_id] += summed_features\n",
    "                weights[:, predicted_class] -= summed_features\n",
    "                biases[label_id] += 1\n",
    "                biases[predicted_class] -= 1\n",
    "\n",
    "    return weights, biases, unique_labels\n",
    "\n",
    "def perceptron_predict(X, weights, biases, unique_labels):\n",
    "    predictions = []\n",
    "\n",
    "    for features in X:\n",
    "        summed_features = np.sum(features, axis=1)\n",
    "        scores = np.dot(summed_features, weights) + biases\n",
    "        predicted_class = np.argmax(scores)\n",
    "        predicted_label = unique_labels[predicted_class]\n",
    "        predictions.append(predicted_label)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def compute_errors(actual_labels, predicted_labels):\n",
    "    sequence_error_count = 0\n",
    "    character_error_count = 0\n",
    "    total_characters = 0\n",
    "\n",
    "    # iterate over each pair of actual and predicted labels\n",
    "    for actual, predicted in zip(actual_labels, predicted_labels):\n",
    "        if not np.array_equal(actual, predicted):  # Compare arrays element-wise\n",
    "            sequence_error_count += 1\n",
    "\n",
    "        for char_actual, char_predicted in zip(actual, predicted):\n",
    "            if char_actual != char_predicted:\n",
    "                character_error_count += 1\n",
    "\n",
    "        total_characters += len(actual)\n",
    "\n",
    "    sequence_error_rate = sequence_error_count / len(actual_labels)\n",
    "    character_error_rate = character_error_count / total_characters\n",
    "\n",
    "    return sequence_error_rate, character_error_rate\n",
    "\n",
    "w, b, lb = perceptron_train(trn_X, trn_Y)\n",
    "Y_pred = perceptron_predict(tst_X, w, b, lb)\n",
    "R_seq, R_char = compute_errors(tst_Y, Y_pred)\n",
    "\n",
    "print(\"Sequence Prediction Error (R_seq):\", R_seq)\n",
    "print(\"Character Prediction Error (R_char):\", R_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 2 (3 points) Implement the Perceptron algorithm for learning parameters (w ∈\n",
    "R\n",
    "d·|A|\n",
    ", b ∈ R\n",
    "|A|, g ∈ R\n",
    "|A|2\n",
    ") of the linear structured output classifier (2). Evaluate the algorithm\n",
    "as specified in Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Prediction Error (R_seq): 0.18\n",
      "Character Prediction Error (R_char): 0.15722379603399433\n"
     ]
    }
   ],
   "source": [
    "def structured_perceptron_train(X, Y, epochs=300):\n",
    "    unique_labels = list(set(Y))\n",
    "    label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    num_features = X[0].shape[0]\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    weights = np.zeros((num_features, num_classes))\n",
    "    biases = np.zeros(num_classes)\n",
    "    g = np.zeros((num_classes, num_classes))  # Structured parameter\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for features, label in zip(X, Y):\n",
    "            label_id = label_to_id[label]\n",
    "\n",
    "            summed_features = np.sum(features, axis=1)\n",
    "\n",
    "            # Calculate scores with the structured output\n",
    "            scores = np.dot(summed_features, weights) + biases\n",
    "            for i in range(num_classes):\n",
    "                for j in range(num_classes):\n",
    "                    scores[i] += g[i][j]  # Incorporate structured parameter\n",
    "\n",
    "            predicted_class = np.argmax(scores)\n",
    "\n",
    "            if predicted_class != label_id:\n",
    "                weights[:, label_id] += summed_features\n",
    "                weights[:, predicted_class] -= summed_features\n",
    "                biases[label_id] += 1\n",
    "                biases[predicted_class] -= 1\n",
    "                \n",
    "                for i in range(num_classes):\n",
    "                    g[label_id][i] += 1\n",
    "                    g[predicted_class][i] -= 1\n",
    "\n",
    "    return weights, biases, g, unique_labels\n",
    "\n",
    "\n",
    "def structured_perceptron_predict(X, weights, biases, g, unique_labels):\n",
    "    predictions = []\n",
    "    for features in X:\n",
    "        summed_features = np.sum(features, axis=1)\n",
    "        scores = np.dot(summed_features, weights) + biases\n",
    "\n",
    "        # Incorporate structured parameter g into the scores\n",
    "        for i in range(len(unique_labels)):\n",
    "            for j in range(len(unique_labels)):\n",
    "                scores[i] += g[i][j]\n",
    "\n",
    "        predicted_class = np.argmax(scores)\n",
    "        predicted_label = unique_labels[predicted_class]\n",
    "        predictions.append(predicted_label)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "w, b, g, lb = structured_perceptron_train(trn_X, trn_Y)\n",
    "Y_pred = structured_perceptron_predict(tst_X, w, b, g, lb)\n",
    "R_seq, R_char = compute_errors(tst_Y, Y_pred)\n",
    "\n",
    "\n",
    "print(\"Sequence Prediction Error (R_seq):\", R_seq)\n",
    "print(\"Character Prediction Error (R_char):\", R_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 3 (3 points) Implement the Perceptron algorithm for learning parameters (w ∈\n",
    "R\n",
    "d·|A|\n",
    ", b ∈ R\n",
    "|A|, v ∈ R\n",
    "|Y|) of the linear structured output classifier (4). Evaluate the algorithm\n",
    "as specified in Assignment 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check (R_seq): 0.0\n",
      "sanity check (R_char): 0.0\n",
      "Sequence Prediction Error (R_seq): 0.13\n",
      "Character Prediction Error (R_char): 0.11567516525023608\n"
     ]
    }
   ],
   "source": [
    "def perceptron_v_train(X, Y, epochs=2000):\n",
    "    unique_labels = list(set(Y))\n",
    "    label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    num_features = X[0].shape[0]\n",
    "    num_classes = len(unique_labels)\n",
    "\n",
    "    weights = np.zeros((num_features, num_classes))\n",
    "    biases = np.zeros(num_classes)\n",
    "    v = np.zeros(len(X), dtype=int)  # Ensure v is of integer type\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        errors = 0\n",
    "        for idx, (features, label) in enumerate(zip(X, Y)):\n",
    "            label_id = label_to_id[label]\n",
    "            summed_features = np.sum(features, axis=1)\n",
    "            scores = np.dot(summed_features, weights) + biases\n",
    "            scores += v[idx]\n",
    "\n",
    "            predicted_class = np.argmax(scores)\n",
    "\n",
    "            if not np.array_equal([predicted_class], [label_id]):  # Convert to arrays for comparison\n",
    "                weights[:, label_id] += summed_features\n",
    "                weights[:, predicted_class] -= summed_features\n",
    "                biases[label_id] += 1\n",
    "                biases[predicted_class] -= 1\n",
    "                v[idx] += 1\n",
    "                errors += 1\n",
    "\n",
    "        if errors == 0:\n",
    "            break\n",
    "\n",
    "    return weights, biases, v\n",
    "\n",
    "def perceptron_v_predict(X, Y, weights, biases, v):\n",
    "    unique_labels = list(set(Y))\n",
    "    predictions = []\n",
    "    for idx, features in enumerate(X):\n",
    "        summed_features = np.sum(features, axis=1)\n",
    "        scores = np.dot(summed_features, weights) + biases\n",
    "        scores += v[idx]\n",
    "\n",
    "        predicted_class = np.argmax(scores)\n",
    "        predicted_label = unique_labels[predicted_class]\n",
    "        predictions.append(predicted_label)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "weights, biases, v = perceptron_v_train(trn_X, trn_Y)\n",
    "Y_pred = perceptron_v_predict(tst_X, trn_Y, weights, biases, v)\n",
    "Y_pred_sanity = perceptron_v_predict(trn_X, trn_Y, weights, biases, v)\n",
    "R_seq, R_char = compute_errors(trn_Y, Y_pred_sanity)\n",
    "\n",
    "print(\"sanity check (R_seq):\", R_seq)\n",
    "print(\"sanity check (R_char):\", R_char)\n",
    "\n",
    "R_seq, R_char = compute_errors(tst_Y, Y_pred)\n",
    "\n",
    "print(\"Sequence Prediction Error (R_seq):\", R_seq)\n",
    "print(\"Character Prediction Error (R_char):\", R_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 4 (1 point) Summarize the testing errors of the three learned classifiers in a\n",
    "single table. Explain differences in the performance of the three classifiers. Point out the main advantages\n",
    "and disadvantages of each classification model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                   | R<sub>seq</sub> | R<sub>char</sub> |\n",
    "|-----------------------------------|----------------|------------------|\n",
    "| independent multi-class classifier| 0.13           | 0.115            |\n",
    "| structured, pair-wise dependency  | 0.18           | 0.157            |\n",
    "| structured, fixed number of sequences | 0.13      | 0.115            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of results** \n",
    "\n",
    "Independent linear multi-class classifier is the simplest one among implemented classifiers. It computes every feasible option independently and is quite fast, however, it completely disregards dependence between characters. I suspect it would be hampered by more complex data, that would for example contain high colinearity between features. It is the only model that does not consider any dependency between features. \n",
    "\n",
    "Structured, pair-wise dependency considers dependences between two characters, allowing more complex model at the cost of higher computational complexity. I think it would work best for data where there is linear relationship between predicted variable and features. \n",
    "\n",
    "As for structured, fixed number of sequences classifier, it predicts sequences with a fixed number of elements, which by itself presumes some relationship between characters. Sequences other than the ones presumed - such as any of the sequences which are not in training dataset, are not possible to be predicted, meaning that model could miss some dependencies between characters which would be present in training data. Therefore the model will likely be best for data with small number of possible sequences, but loses it's advantages with increasing number of possible sequences.\n",
    "\n",
    "The Indpendent multi-class classifier and structured, fixed number of sequences classifier both reach the same accuracy of classification. Also, they both classify as one from the set of labels given in training data. This suggests to me that both classifiers likely reached perfect score for the training data (structured, fixed sequence has 0 error rate on training data for sure), and are classifying testing data in the same manner.\n",
    "\n",
    "Structured, pair-wise dependency is worse at classifying sequences, but better at characters. This is likely because this classifier model is focused in particullar on relationships between singular characters, likely at the cost of sequence accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 5 (5 bonus points) Describe an instance of the Structured Output SVM algorithm for learning the classifier (2) which uses the character prediction error Rchar as the target\n",
    "loss function. Learn the classifier from the training data and report its test performance in\n",
    "terms of the sequence prediction error Rseq and the character prediction error Rchar\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input of Structured Output SVM (Structured Vector Machine) is binary image as per second point of our assignment. It is a linear classifier designed for sequence prediction tasks. Each sequence is a string of characters, and the goal is to correctly predict these sequences. \n",
    "\n",
    "SVMs use loss function to quantify the departure of prediction from the actual output variable. Rchar handily slots into this utility, since its one of two metrics we were using to monitor performance. We will try to minimise loss function - model's Rchar.\n",
    "\n",
    "I was not explicitly told to implement this SVM myself, so I assume I can use libraries for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating eror to 0.012027491408934709\n",
      "updating eror to 0.012027491408934709\n",
      "updating eror to 0.012027491408934709\n",
      "updating eror to 0.012027491408934709\n",
      "updating eror to 0.012027491408934709\n",
      "updating eror to 0.012027491408934709\n",
      "updating eror to 0.012027491408934709\n",
      "updating eror to 0.012027491408934709\n",
      "updating eror to 0.012027491408934709\n",
      "updating eror to 0.012027491408934709\n",
      "Predicted words: ['raleh', 'fioya', 'raleh', 'fiosb', 'raieh', 'ciiytord', 'won', 'raifh', 'tioyd', 'dwlgft', 'raipb', 'ralph', 'hulnn', 'drwck', 'fhllip', 'rioyd', 'pliiip', 'fioyd', 'cuipk', 'yg', 'dwldht', 'ralpu', 'fiogd', 'dwlght', 'fioyd', 'ty', 'quinn', 'ty', 'ceuz', 'steve', 'joseeh', 'ty', 'mdx', 'jacp', 'max', 'dwisdt', 'bc', 'crwz', 'dwipht', 'mox', 'ty', 'bxouc', 'nninn', 'eluij', 'ralyh', 'quinn', 'dwight', 'ftoyd', 'nax', 'brock', 'jark', 'qulnn', 'bo', 'cruz', 'crdz', 'ralph', 'ho', 'floyd', 'joseph', 'ty', 'yreva', 'oo', 'ty', 'ty', 'ralph', 'steve', 'drdw', 'max', 'jaau', 'tg', 'drew', 'dwiyht', 'ralph', 'brock', 'crut', 'bo', 'joseph', 'ralph', 'qvinn', 'qnimw', 'by', 'jacu', 'crue', 'max', 'max', 'cruc', 'flold', 'mnx', 'ckuz', 'dwlgnt', 'tyoyd', 'tloyd', 'ty', 'cruz', 'po', 'fgoyd', 'rloyd', 'tg', 'max', 'floxd', 'ty', 'kcnyd', 'max', 'quinn', 'ralph', 'elvis', 'grey', 'arcz', 'iloyd', 'hvgh', 'cruz', 'qninn', 'stfye', 'tg', 'may', 'crua', 'cruz', 'bo', 'mau', 'gaeg', 'do', 'gnimn', 'quinn', 'floyd', 'quinn', 'bxvck', 'hhgh', 'drew', 'ty', 'steva', 'cruz', 'quinn', 'dvidhl', 'qulnn', 'ty', 'dwigkd', 'jobapk', 'dwtyht', 'max', 'max', 'quiwn', 'eluis', 'quinn', 'jack', 'dwlaht', 'steve', 'zloyd', 'quinn', 'cruz', 'cruz', 'jack', 'dwight', 'jacn', 'uock', 'baock', 'droch', 'crvz', 'floyd', 'jnck', 'joreyk', 'rglrh', 'clircord', 'dwiaht', 'quinn', 'floyd', 'bo', 'ty', 'dwigtt', 'guinn', 'tt', 'jack', 'ho', 'cruz', 'ccuz', 'dwiqht', 'brack', 'ralph', 'steve', 'bicck', 'ty', 'ty', 'cruz', 'quinn', 'ts', 'crvz', 'yclth', 'jack', 'bo', 'max', 'qutnn', 'ty', 'max', 'jnck', 'qminn', 'max', 'pkclip', 'floyd', 'ty', 'quinn', 'ouinn', 'cruz', 'elvls', 'bo', 'floyd', 'quinu', 'ty', 'huyh', 'fioyd', 'dwiqht', 'qulnn', 'stcve', 'brock', 'goseph', 'jaok', 'dwiyhr', 'rlogd', 'max', 'jack', 'mrx', 'rmqqh', 'quinn', 'quynn', 'srck', 'josaph', 'mat', 'flogd', 'elvig', 'floyd', 'mnx', 'steve', 'eluib', 'mox', 'elvis', 'max', 'cruz', 'broct', 'dwiqht', 'floyd', 'floyd', 'mox', 'crdz', 'eluis', 'jack', 'dwight', 'devyn', 'uoseph', 'fteve', 'dwiyht', 'jacr', 'mdx', 'nax', 'ty', 'max', 'quinn', 'bo', 'bg', 'ty', 'philip', 'brock', 'josepn', 'owlght', 'tq', 'max', 'tteve', 'qvinn', 'jdck', 'hioyd', 'bo', 'dioyd', 'cruz', 'jacb', 'wax', 'devyn', 'bo', 'dwiehy', 'ty', 'cruz', 'ho', 'bfoce', 'bo', 'fioyd', 'oruz', 'bo', 'cruq', 'cruj', 'rloyd', 'bo', 'jack', 'qreg', 'qy', 'quinn', 'crut', 'max', 'ty', 'stevt', 'gteve', 'bo', 'floyd', 'josepx', 'steve', 'dwighb', 'hagh', 'jmcx', 'qalnu', 'drew', 'cyuz', 'huxh', 'oevyq', 'max', 'huqh', 'iy', 'btuch', 'steue', 'du', 'ho', 'flogd', 'stuve', 'dnighe', 'ho', 'quinn', 'jach', 'jnck', 'iy', 'clifford', 'tloyd', 'hugd', 'ty', 'max', 'crnz', 'ehoyd', 'steve', 'td', 'dryw', 'rloyd', 'dloyd', 'fioyd', 'floyd', 'steuz', 'tloyd', 'mav', 'crut', 'do', 'max', 'cvvz', 'oevyn', 'brock', 'quinn', 'shebe', 'jach', 'max', 'rdlph', 'rkoxd', 'dwight', 'cfaz', 'quinn', 'cruz', 'flouj', 'tloyd', 'guinn', 'bo', 'qcimn', 'cruz', 'floyd', 'ploya', 'philip', 'rteve', 'philip', 'mar', 'mav', 'max', 'quinn', 'qoinn', 'goseph', 'ux', 'cyuz', 'bo', 'eluis', 'dacu', 'turnn', 'max', 'ifcx', 'josepb', 'floyd', 'ralph', 'dojryh', 'vy', 'bo', 'jdch', 'ty', 'ccve', 'drtw', 'dwight', 'craz', 'tloyd', 'tax', 'dwlohe', 'greg', 'crue', 'gteve', 'maf', 'jack', 'mnx', 'steve', 'tloyo', 'ho', 'hush', 'dwigkf', 'sy', 'fcoyd', 'rloqd', 'ralqh', 'voscpn', 'jack', 'bugh', 'cruz', 'ywighj', 'cruz', 'fioyd', 'bo', 'max', 'ludf', 'akviq', 'floyd', 'crue', 'ty', 'ho', 'pmiilp', 'dvcw', 'duinn', 'ty', 'phoyd', 'philip', 'tg', 'do', 'orus', 'hugh', 'broek', 'joseph', 'dwight', 'dcvyn', 'dvew', 'duight', 'ernz', 'phiiie', 'dwioht', 'ioseph', 'qked', 'joseph', 'tg', 'jaex', 'jogeph', 'jacd', 'jock', 'max', 'ccuz', 'ralph', 'doseeh', 'bo', 'xy', 'joseph', 'quiuz', 'crqz', 'dtgyd', 'cruz', 'tu', 'crvt', 'max', 'deryv', 'sloyd', 'max', 'crut', 'max', 'floyd', 'drer', 'gteve', 'yy', 'ralpk', 'drew', 'dvew', 'crus', 'hugb', 'floyd', 'eivis', 'bo', 'ralph', 'flood', 'mdx', 'cruz', 'puinn', 'seeve', 'elvis', 'ordak', 'smeve', 'tx', 'yuinn', 'jock', 'cruz', 'jeunn', 'dwight', 'cruy']\n",
      "Proper words: ['ralph', 'floyd', 'ralph', 'floyd', 'ralph', 'clifford', 'max', 'ralph', 'floyd', 'dwight', 'ralph', 'ralph', 'quinn', 'brock', 'philip', 'floyd', 'philip', 'floyd', 'ralph', 'ty', 'dwight', 'ralph', 'floyd', 'dwight', 'floyd', 'ty', 'quinn', 'ty', 'cruz', 'steve', 'joseph', 'ty', 'max', 'jack', 'max', 'dwight', 'bo', 'cruz', 'dwight', 'max', 'ty', 'brock', 'quinn', 'elvis', 'ralph', 'quinn', 'dwight', 'floyd', 'max', 'brock', 'jack', 'quinn', 'bo', 'cruz', 'cruz', 'ralph', 'bo', 'floyd', 'joseph', 'ty', 'steve', 'bo', 'ty', 'ty', 'ralph', 'steve', 'drew', 'max', 'jack', 'ty', 'drew', 'dwight', 'ralph', 'brock', 'cruz', 'bo', 'joseph', 'ralph', 'quinn', 'quinn', 'ty', 'jack', 'cruz', 'max', 'max', 'cruz', 'floyd', 'max', 'cruz', 'dwight', 'floyd', 'floyd', 'ty', 'cruz', 'bo', 'floyd', 'floyd', 'ty', 'max', 'floyd', 'ty', 'devyn', 'max', 'quinn', 'ralph', 'elvis', 'greg', 'cruz', 'floyd', 'hugh', 'cruz', 'quinn', 'steve', 'ty', 'max', 'cruz', 'cruz', 'bo', 'max', 'greg', 'bo', 'quinn', 'quinn', 'floyd', 'quinn', 'brock', 'hugh', 'drew', 'ty', 'steve', 'cruz', 'quinn', 'dwight', 'quinn', 'ty', 'dwight', 'joseph', 'dwight', 'max', 'max', 'quinn', 'elvis', 'quinn', 'jack', 'dwight', 'steve', 'floyd', 'quinn', 'cruz', 'cruz', 'jack', 'dwight', 'jack', 'jack', 'brock', 'brock', 'cruz', 'floyd', 'jack', 'joseph', 'ralph', 'clifford', 'dwight', 'quinn', 'floyd', 'bo', 'ty', 'dwight', 'quinn', 'ty', 'jack', 'bo', 'cruz', 'cruz', 'dwight', 'brock', 'ralph', 'steve', 'brock', 'ty', 'ty', 'cruz', 'quinn', 'ty', 'cruz', 'ralph', 'jack', 'bo', 'max', 'quinn', 'ty', 'max', 'jack', 'quinn', 'max', 'philip', 'floyd', 'ty', 'quinn', 'quinn', 'cruz', 'elvis', 'bo', 'floyd', 'quinn', 'ty', 'hugh', 'floyd', 'dwight', 'quinn', 'steve', 'brock', 'joseph', 'jack', 'dwight', 'floyd', 'max', 'jack', 'max', 'ralph', 'quinn', 'quinn', 'jack', 'joseph', 'max', 'floyd', 'elvis', 'floyd', 'max', 'steve', 'elvis', 'max', 'elvis', 'max', 'cruz', 'brock', 'dwight', 'floyd', 'floyd', 'max', 'cruz', 'elvis', 'jack', 'dwight', 'devyn', 'joseph', 'steve', 'dwight', 'jack', 'max', 'max', 'ty', 'max', 'quinn', 'bo', 'bo', 'ty', 'philip', 'brock', 'joseph', 'dwight', 'ty', 'max', 'steve', 'quinn', 'jack', 'floyd', 'bo', 'floyd', 'cruz', 'jack', 'max', 'devyn', 'bo', 'dwight', 'ty', 'cruz', 'bo', 'brock', 'bo', 'floyd', 'cruz', 'bo', 'cruz', 'cruz', 'floyd', 'bo', 'jack', 'greg', 'ty', 'quinn', 'cruz', 'max', 'ty', 'steve', 'steve', 'bo', 'floyd', 'joseph', 'steve', 'dwight', 'hugh', 'jack', 'quinn', 'drew', 'cruz', 'hugh', 'devyn', 'max', 'hugh', 'ty', 'brock', 'steve', 'bo', 'bo', 'floyd', 'steve', 'dwight', 'bo', 'quinn', 'jack', 'jack', 'ty', 'clifford', 'floyd', 'hugh', 'ty', 'max', 'cruz', 'floyd', 'steve', 'ty', 'drew', 'floyd', 'floyd', 'floyd', 'floyd', 'steve', 'floyd', 'max', 'cruz', 'bo', 'max', 'cruz', 'devyn', 'brock', 'quinn', 'steve', 'jack', 'max', 'ralph', 'floyd', 'dwight', 'cruz', 'quinn', 'cruz', 'floyd', 'floyd', 'quinn', 'bo', 'quinn', 'cruz', 'floyd', 'floyd', 'philip', 'steve', 'philip', 'max', 'max', 'max', 'quinn', 'quinn', 'joseph', 'ty', 'cruz', 'bo', 'elvis', 'jack', 'quinn', 'max', 'jack', 'joseph', 'floyd', 'ralph', 'joseph', 'ty', 'bo', 'jack', 'ty', 'cruz', 'drew', 'dwight', 'cruz', 'floyd', 'max', 'dwight', 'greg', 'cruz', 'steve', 'max', 'jack', 'max', 'steve', 'floyd', 'bo', 'hugh', 'dwight', 'ty', 'floyd', 'floyd', 'ralph', 'joseph', 'jack', 'hugh', 'cruz', 'dwight', 'cruz', 'floyd', 'bo', 'max', 'hugh', 'elvis', 'floyd', 'cruz', 'ty', 'bo', 'philip', 'drew', 'quinn', 'ty', 'floyd', 'philip', 'ty', 'bo', 'cruz', 'hugh', 'brock', 'joseph', 'dwight', 'devyn', 'drew', 'dwight', 'cruz', 'philip', 'dwight', 'joseph', 'greg', 'joseph', 'ty', 'jack', 'joseph', 'jack', 'jack', 'max', 'cruz', 'ralph', 'joseph', 'bo', 'ty', 'joseph', 'quinn', 'cruz', 'floyd', 'cruz', 'ty', 'cruz', 'max', 'devyn', 'floyd', 'max', 'cruz', 'max', 'floyd', 'drew', 'steve', 'ty', 'ralph', 'drew', 'drew', 'cruz', 'hugh', 'floyd', 'elvis', 'bo', 'ralph', 'floyd', 'max', 'cruz', 'quinn', 'steve', 'elvis', 'brock', 'steve', 'ty', 'quinn', 'jack', 'cruz', 'devyn', 'dwight', 'cruz']\n",
      "Sequence Prediction Error (R_seq): 0.63\n",
      "Character Prediction Error (R_char): 0.21104815864022664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def flatten_training_data(X, Y):\n",
    "    flattened_data = []\n",
    "    labels = []\n",
    "\n",
    "    for i, (data_point, word) in enumerate(zip(X, Y)):\n",
    "        for char_index in range(len(word)):\n",
    "            # Collect the char_index-th element from each of the 8256 features\n",
    "            char_features = data_point[:, char_index]\n",
    "            flattened_data.append(char_features)\n",
    "            labels.append(word[char_index])\n",
    "\n",
    "    return np.array(flattened_data), np.array(labels)\n",
    "\n",
    "def compute_character_error(actual_labels, predicted_labels):\n",
    "    character_error_count = 0\n",
    "    for actual, predicted in zip(actual_labels, predicted_labels):\n",
    "        if actual != predicted:\n",
    "            character_error_count += 1\n",
    "    return character_error_count / len(actual_labels)\n",
    "\n",
    "def train_svm_with_custom_loss(X, Y, epochs=10, tolerance=0.001):\n",
    "    best_model = None\n",
    "    lowest_char_error = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model = LinearSVC(random_state=epoch, tol=1e-5)\n",
    "        model.fit(X, Y)\n",
    "        Y_pred = model.predict(X)\n",
    "\n",
    "        char_error = compute_character_error(Y, Y_pred)\n",
    "        if char_error <= lowest_char_error:\n",
    "            print(f\"updating eror to {char_error}\")\n",
    "            lowest_char_error = char_error\n",
    "            best_model = model\n",
    "\n",
    "        if char_error < tolerance:\n",
    "            print(\"tolerand enough\")\n",
    "            break\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def predict_and_evaluate(model, tst_X, tst_Y):\n",
    "    predicted_words = []\n",
    "    for data_point in tst_X:\n",
    "        predicted_word = ''\n",
    "        for char_index in range(data_point.shape[1]):\n",
    "            char_features = data_point[:, char_index]\n",
    "            predicted_char = model.predict([char_features])[0]\n",
    "            predicted_word += predicted_char\n",
    "        predicted_words.append(predicted_word)\n",
    "    \n",
    "    R_seq, R_char = compute_errors(tst_Y, predicted_words)\n",
    "    return predicted_words, R_seq, R_char\n",
    "\n",
    "flattened_data, labels = flatten_training_data(trn_X, trn_Y)\n",
    "model = train_svm_with_custom_loss(flattened_data, labels)\n",
    "predicted_words, R_seq, R_char = predict_and_evaluate(model, tst_X, tst_Y)\n",
    "\n",
    "print(\"Predicted words:\", predicted_words)\n",
    "print(\"Proper words:\", tst_Y)\n",
    "print(\"Sequence Prediction Error (R_seq):\", R_seq)\n",
    "print(\"Character Prediction Error (R_char):\", R_char)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
