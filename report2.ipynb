{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report - Lab: SO Perceptron #\n",
    "\n",
    "* name: Filip Špidla \n",
    "\n",
    " * email: spidlfil@fel.cvut.cz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# load single example\n",
    "def load_example( img_path ):\n",
    "\n",
    "    Y = img_path[img_path.rfind('_')+1:-4]\n",
    "\n",
    "    img = Image.open( img_path )\n",
    "    img_mat = np.asarray( img )\n",
    "    \n",
    "    n_letters = len( Y )\n",
    "    im_height = int(img_mat.shape[0])\n",
    "    im_width = int(img_mat.shape[1]/n_letters)\n",
    "    n_pixels = im_height*im_width\n",
    "    \n",
    "    X = np.zeros( [int(n_pixels+n_pixels*(n_pixels-1)/2),n_letters])\n",
    "    for i in range(n_letters):\n",
    "        \n",
    "        # single letter\n",
    "        letter = img_mat[:,i*im_width:(i+1)*im_width]/255\n",
    "        \n",
    "        # compute features\n",
    "        x = letter.flatten()\n",
    "        X[0:len(x),i] = x\n",
    "        cnt = n_pixels \n",
    "        for j in range(0,n_pixels-1):\n",
    "            for k in range(j+1,n_pixels):\n",
    "                X[cnt,i] = x[j]*x[k]\n",
    "                cnt = cnt + 1\n",
    "           \n",
    "        X[:,i] = X[:,i]/np.linalg.norm(X[:,i])\n",
    "        \n",
    "    return X, Y, img\n",
    "    \n",
    "# load all examples from a folder    \n",
    "def load_examples( image_folder ):\n",
    "    \n",
    "    files = [f for f in listdir(image_folder) if isfile(join(image_folder, f))]\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    img = []\n",
    "    for file in listdir(image_folder):\n",
    "        path = join(image_folder, file)\n",
    "        if isfile( path ):\n",
    "                        \n",
    "            X_,Y_,img_ = load_example( path )\n",
    "            X.append( X_ )\n",
    "            Y.append( Y_ )\n",
    "            img.append( img_ )\n",
    "        \n",
    "    return X, Y, img\n",
    "    \n",
    "# load training examples\n",
    "trn_X, trn_Y, trn_img = load_examples( 'ocr_names_images/trn' )\n",
    "\n",
    "# load testing examples\n",
    "tst_X, tst_Y, tst_img = load_examples( 'ocr_names_images/tst' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 (3 points) ## \n",
    "Implement the Perceptron algorithm for learning parameters (w ∈\n",
    "R\n",
    "d·|A|\n",
    ", b ∈ R\n",
    "|A|) of the linear multi-class classifier (1). Use the provided training examples T\n",
    "m\n",
    "to learn parameters of the classifier. Report the sequence prediction error Rseq and the character\n",
    "prediction error Rchar computed on the provided testing examples S\n",
    "l\n",
    ". The output should be a\n",
    "single script (Jupyter notebook or Matlab) which learns the classifier and prints the computed\n",
    "testing errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Prediction Error (R_char): 0.27478753541076484\n",
      "Sequence Prediction Error (R_seq): 0.73\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        self.weights = np.zeros((n_features, n_classes))\n",
    "        self.biases = np.zeros(n_classes)\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = np.dot(X, self.weights) + self.biases\n",
    "        predictions = np.argmax(scores, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def train(self, X, Y, epochs=10):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                for j in range(X[i].shape[1]):  # Iterate through each character in the name\n",
    "                    x = X[i][:, j]  # Features for one character\n",
    "                    char = Y[i][j]\n",
    "                    if 'a' <= char <= 'z':\n",
    "                        y_true = ord(char) - ord('a') \n",
    "                    else:\n",
    "                        print(f\"Unexpected character: {char}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Prediction\n",
    "                    y_pred = self.predict(x.reshape(1, -1))[0]\n",
    "                    \n",
    "                    # Perceptron update rule\n",
    "                    if y_pred != y_true:\n",
    "                        self.weights[:, y_pred] -= x  # Decrease weight of wrong prediction\n",
    "                        self.weights[:, y_true] += x  # Increase weight of correct class\n",
    "                        self.biases[y_pred] -= 1\n",
    "                        self.biases[y_true] += 1\n",
    "                        \n",
    "    def generate_predictions(self, X):\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            sequence_predictions = ''\n",
    "            for j in range(X[i].shape[1]):\n",
    "                x = X[i][:, j]\n",
    "                y_pred = self.predict(x.reshape(1, -1))[0]\n",
    "                char_pred = chr(y_pred + ord('a'))  # Decode numerical prediction to character\n",
    "                sequence_predictions += char_pred\n",
    "            predictions.append(sequence_predictions)\n",
    "        return predictions\n",
    "\n",
    "    def evaluate_predictions(self, predictions, Y):\n",
    "        total_chars = 0\n",
    "        correct_predictions = 0\n",
    "        correct_sequences = 0\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            sequence_correct = True\n",
    "            for j in range(len(predictions[i])):\n",
    "                y_pred = predictions[i][j]\n",
    "                y_true = Y[i][j]\n",
    "\n",
    "                if y_pred == y_true:\n",
    "                    correct_predictions += 1\n",
    "                else:\n",
    "                    sequence_correct = False\n",
    "                total_chars += 1\n",
    "\n",
    "            if sequence_correct:\n",
    "                correct_sequences += 1\n",
    "\n",
    "        R_char = 1 - (correct_predictions / total_chars)\n",
    "        R_seq = 1 - (correct_sequences / len(Y))\n",
    "        return R_char, R_seq\n",
    "\n",
    "n_features = trn_X[0].shape[0]\n",
    "unique_chars = set(''.join(trn_Y))\n",
    "n_classes = len(unique_chars)  \n",
    "\n",
    "# Initialize and train the Perceptron\n",
    "perceptron = Perceptron(n_features, n_classes)\n",
    "perceptron.train(trn_X, trn_Y)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = perceptron.generate_predictions(tst_X)\n",
    "\n",
    "# Evaluate the predictions\n",
    "R_char, R_seq = perceptron.evaluate_predictions(predictions, tst_Y)\n",
    "print(\"Character Prediction Error (R_char):\", R_char)\n",
    "print(\"Sequence Prediction Error (R_seq):\", R_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 (3 points) ## \n",
    "Implement the Perceptron algorithm for learning parameters (w ∈\n",
    "R\n",
    "d·|A|\n",
    ", b ∈ R\n",
    "|A|, g ∈ R\n",
    "|A|2\n",
    ") of the linear structured output classifier (2). Evaluate the algorithm\n",
    "as specified in Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Prediction Error (R_char): 0.0882908404154863\n",
      "Sequence Prediction Error (R_seq): 0.23199999999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class StructuredPerceptron:\n",
    "    def __init__(self, n_features, n_classes):\n",
    "        self.weights = np.zeros((n_features, n_classes))\n",
    "        self.biases = np.zeros(n_classes)\n",
    "        self.pairwise_weights = np.zeros((n_classes, n_classes))\n",
    "\n",
    "    def predict_sequence(self, X):\n",
    "        L = X.shape[1]\n",
    "        A = self.weights.shape[1]\n",
    "        F = np.zeros((L, A))\n",
    "        Y = np.zeros((L, A), dtype=int)\n",
    "\n",
    "        # Compute the first F values\n",
    "        F[0] = np.dot(X[:, 0].T, self.weights) + self.biases\n",
    "\n",
    "        # Dynamic programming for the rest\n",
    "        for i in range(1, L):\n",
    "            for y in range(A):\n",
    "                scores = F[i-1] + self.pairwise_weights[:, y] + np.dot(X[:, i].T, self.weights[:, y]) + self.biases[y]\n",
    "                F[i, y] = np.max(scores)\n",
    "                Y[i, y] = np.argmax(scores)\n",
    "\n",
    "        # Backtrack to find the sequence\n",
    "        y_pred = np.zeros(L, dtype=int)\n",
    "        y_pred[-1] = np.argmax(F[-1])\n",
    "        for i in range(L - 2, -1, -1):\n",
    "            y_pred[i] = Y[i + 1, y_pred[i + 1]]\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y, epochs=30):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                y_pred = self.predict_sequence(X[i])\n",
    "\n",
    "                for j in range(X[i].shape[1]):\n",
    "                    x = X[i][:, j]\n",
    "                    char = Y[i][j]\n",
    "                    if 'a' <= char <= 'z':\n",
    "                        y_true = ord(char) - ord('a')\n",
    "                    else:\n",
    "                        print(f\"Unexpected character: {char}\")\n",
    "                        continue\n",
    "\n",
    "                    if y_pred[j] != y_true:\n",
    "                        self.weights[:, y_pred[j]] -= x\n",
    "                        self.weights[:, y_true] += x\n",
    "                        self.biases[y_pred[j]] -= 1\n",
    "                        self.biases[y_true] += 1\n",
    "\n",
    "                        if j > 0:\n",
    "                            # Update pairwise weights\n",
    "                            self.pairwise_weights[y_pred[j-1], y_pred[j]] -= 1\n",
    "                            self.pairwise_weights[y_pred[j-1], y_true] += 1\n",
    "\n",
    "    def generate_predictions(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for sequence in X:\n",
    "            y_pred = self.predict_sequence(sequence)\n",
    "            \n",
    "            decoded = ''\n",
    "            for char in y_pred:\n",
    "                decoded += chr(char + ord('a'))  \n",
    "\n",
    "            predictions.append(decoded)\n",
    "\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate_predictions(self, predictions, Y):\n",
    "        total_chars = 0\n",
    "        correct_predictions = 0\n",
    "        correct_sequences = 0\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            sequence_correct = True\n",
    "            for j in range(len(predictions[i])):\n",
    "                y_pred = predictions[i][j]\n",
    "                y_true = Y[i][j]\n",
    "\n",
    "                if y_pred == y_true:\n",
    "                    correct_predictions += 1\n",
    "                else:\n",
    "                    sequence_correct = False\n",
    "                total_chars += 1\n",
    "\n",
    "            if sequence_correct:\n",
    "                correct_sequences += 1\n",
    "\n",
    "        R_char = 1 - (correct_predictions / total_chars)\n",
    "        R_seq = 1 - (correct_sequences / len(Y))\n",
    "        return R_char, R_seq\n",
    "\n",
    "structured_perceptron = StructuredPerceptron(n_features, n_classes)\n",
    "structured_perceptron.train(trn_X, trn_Y)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = structured_perceptron.generate_predictions(tst_X)\n",
    "\n",
    "# Evaluate the predictions\n",
    "R_char, R_seq = structured_perceptron.evaluate_predictions(predictions, tst_Y)\n",
    "print(\"Character Prediction Error (R_char):\", R_char)\n",
    "print(\"Sequence Prediction Error (R_seq):\", R_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3 (3 points) ## \n",
    "Implement the Perceptron algorithm for learning parameters (w ∈\n",
    "R\n",
    "d·|A|\n",
    ", b ∈ R\n",
    "|A|, v ∈ R\n",
    "|Y|) of the linear structured output classifier (4). Evaluate the algorithm\n",
    "as specified in Assignment 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Prediction Error (R_char): 0.02596789423984891\n",
      "Sequence Prediction Error (R_seq): 0.028000000000000025\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class FixedSequencePerceptron:\n",
    "    def __init__(self, n_features, n_classes, n_sequences, trn_Y):\n",
    "        self.weights = np.zeros((n_features, n_classes))\n",
    "        self.biases = np.zeros(n_classes)\n",
    "        self.sequence_weights = np.zeros(n_sequences)  # Weights for entire sequences\n",
    "        self.sequences = list(set(trn_Y))\n",
    "\n",
    "    def predict_sequence(self, X, sequences):\n",
    "        max_score = float('-inf')\n",
    "        best_sequence = None\n",
    "\n",
    "        for idx, seq in enumerate(sequences):\n",
    "            score = 0\n",
    "            for i, char in enumerate(seq):\n",
    "                if i < X.shape[1]:  \n",
    "                    score += np.dot(X[:, i].T, self.weights[:, ord(char) - ord('a')]) + self.biases[ord(char) - ord('a')]\n",
    "            score += self.sequence_weights[idx]\n",
    "\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_sequence = seq\n",
    "\n",
    "        return best_sequence\n",
    "\n",
    "    def train(self, X, Y, epochs=30):\n",
    "        sequences =  list(set(trn_Y))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                y_pred = self.predict_sequence(X[i], sequences)\n",
    "                y_true = Y[i]\n",
    "\n",
    "                min_len = min(len(y_pred), len(y_true))\n",
    "\n",
    "                if y_pred[:min_len] != y_true[:min_len]:\n",
    "                    for j in range(min_len):\n",
    "                        y_true_idx = ord(y_true[j]) - ord('a')\n",
    "                        y_pred_idx = ord(y_pred[j]) - ord('a')\n",
    "                        self.weights[:, y_true_idx] += X[i][:, j]\n",
    "                        self.weights[:, y_pred_idx] -= X[i][:, j]\n",
    "                        self.biases[y_true_idx] += 1\n",
    "                        self.biases[y_pred_idx] -= 1\n",
    "\n",
    "                    seq_true_idx = sequences.index(y_true)\n",
    "                    seq_pred_idx = sequences.index(y_pred)\n",
    "                    self.sequence_weights[seq_true_idx] += 1\n",
    "                    self.sequence_weights[seq_pred_idx] -= 1\n",
    "\n",
    "    def generate_predictions(self, X):\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(len(X)):\n",
    "            prediction = self.predict_sequence(X[i], self.sequences)\n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "\n",
    "    def evaluate_predictions(self, predictions, Y):\n",
    "        correct_sequences = 0\n",
    "        total_chars = 0\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            total_chars += len(Y[i])\n",
    "            y_pred = predictions[i]\n",
    "            if y_pred == Y[i]:\n",
    "                correct_sequences += 1\n",
    "                correct_predictions += len(Y[i])\n",
    "\n",
    "        R_seq = 1 - (correct_sequences / len(predictions))\n",
    "        R_char = 1 - (correct_predictions / total_chars)\n",
    "        return R_char, R_seq\n",
    "\n",
    "n_features = trn_X[0].shape[0]\n",
    "unique_chars = set(''.join(trn_Y))\n",
    "n_classes = len(unique_chars)\n",
    "# Number of unique sequences in the dataset\n",
    "n_sequences = len(set(trn_Y))\n",
    "\n",
    "fixed_seq_perceptron = FixedSequencePerceptron(n_features, n_classes, n_sequences, trn_Y)\n",
    "fixed_seq_perceptron.train(trn_X, trn_Y)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = fixed_seq_perceptron.generate_predictions(tst_X)\n",
    "\n",
    "# Evaluate the predictions\n",
    "R_char, R_seq = fixed_seq_perceptron.evaluate_predictions(predictions, tst_Y)\n",
    "print(\"Character Prediction Error (R_char):\", R_char)\n",
    "print(\"Sequence Prediction Error (R_seq):\", R_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 (1 point) ##\n",
    "Summarize the testing errors of the three learned classifiers in a\n",
    "single table. Explain differences in the performance of the three classifiers. Point out the main advantages\n",
    "and disadvantages of each classification model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                   | R<sub>seq</sub> | R<sub>char</sub> |\n",
    "|-----------------------------------|----------------|------------------|\n",
    "| independent multi-class classifier| 0.73           | 0.274           |\n",
    "| structured, pair-wise dependency  | 0.231          | 0.088           |\n",
    "| structured, fixed number of sequences | 0.028      | 0.026            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of results** \n",
    "\n",
    "Independent linear multi-class classifier is the simplest one among implemented classifiers. It computes every feasible option independently and is quite fast to train, however, it completely disregards dependence between characters. In fact, it is the only model used that does not consider any dependency between features. R<sub>seq</sub> is quite high, and it would most likely be even higher with longer average lenght of predicted sequence. R<sub>char</sub> is better, but bad in comparison to other models. I suspect that in general, it would be hampered by more complex data, that would for example contain high colinearity between features. \n",
    "\n",
    "Structured, pair-wise dependency considers dependences between two characters, allowing more complex model at the cost of higher computational complexity. This focus on dependency significantly increases model's performance, suggesting that some pairs are more likely to occur that others - which makes sense, given there are 20 different words in total. Some pairs of characters are more likely to occur, while others do not occur at all. I think that in general, it would work best for data where there are linear relationship between characters - since the matrix used captures linear relations. \n",
    "\n",
    "As for structured, fixed number of sequences classifier, it predicts sequences with a fixed characters, which by itself is a relationship between characters (as in, these characters for a sequence, and others do not). Sequences other than the ones presumed - such as any of the sequences which are not in training dataset, are not possible to be predicted, meaning that model could miss some dependencies between characters which would be present in training data. Therefore the model will likely be best for data with small number of possible sequences, but loses it's advantages with increasing number of possible sequences. As far as I could tell, all the possible sequences from testing data were present in training data, so this weakness would not be exploited. \n",
    "\n",
    "Interestingly, contrary to the other models, it has near identical R<sub>seq</sub> and R<sub>char</sub>. However, in a dataset with varying lenghts of predicted sequences, I would expect R<sub>char</sub> to get smaller in comparison to R<sub>seq</sub>, since I would expect the longer sequences to be easier to correctly predict, while short sequences offer less parameters to predict. This should result in larger number of smaller sequences to be incorrectly predicted, resulting in more incorrect sequences rather than incorrect characters. \n",
    "\n",
    "In the end, structured classifier with fixed number of sequences has by far the best results for this dataset. However, it should be noted that this dataset has small amount of possible sequences which lie on a normal distribution. This will make it more profitable to treat the sequence of characters as a whole and focus on their relations, rather than predicting them individualy. Hovewer in more complex data with less simple relations, greater number of possible sequences etc, I would think that structured, pair-wise dependency, and after a while. indpenendent multi-class classifier, could prove to be the better choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5 (5 bonus points) ##\n",
    " Describe an instance of the Structured Output SVM algorithm for learning the classifier (2) which uses the character prediction error Rchar as the target\n",
    "loss function. Learn the classifier from the training data and report its test performance in\n",
    "terms of the sequence prediction error Rseq and the character prediction error Rchar\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "\n",
    "Input of Structured Output SVM (Structured Vector Machine) is binary image as per second point of our assignment. It is a linear classifier designed for sequence prediction tasks. Each sequence is a string of characters, and the goal is to correctly predict characters of the sequence. \n",
    "\n",
    "SVMs use loss function to quantify the departure of prediction from the actual output variable. R<sub>char</sub> handily slots into this utility, since its one of two metrics we were using to monitor performance. We will try to minimise loss function - model's R<sub>char</sub>.\n",
    "\n",
    "I was not explicitly told to implement this SVM myself, so I assume I can use libraries for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Prediction Error (R_char): 0.07082152974504252\n",
      "Sequence Prediction Error (R_seq): 0.134\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class StructuredSVM:\n",
    "    def __init__(self, n_features, n_classes, C=1.0):\n",
    "        self.weights = np.zeros((n_features, n_classes))\n",
    "        self.biases = np.zeros(n_classes)\n",
    "        self.pairwise_weights = np.zeros((n_classes, n_classes))\n",
    "        self.C = C  # Regularization parameter\n",
    "\n",
    "    def char_to_index(self, char):\n",
    "        return ord(char) - ord('a')\n",
    "\n",
    "    def index_to_char(self, index):\n",
    "        return chr(index + ord('a'))\n",
    "\n",
    "    def predict_sequence(self, X):\n",
    "        L = X.shape[1]\n",
    "        A = self.weights.shape[1]\n",
    "        F = np.zeros((L, A))\n",
    "        Y = np.zeros((L, A), dtype=int)\n",
    "\n",
    "        F[0] = np.dot(X[:, 0].T, self.weights) + self.biases\n",
    "\n",
    "        for i in range(1, L):\n",
    "            for y in range(A):\n",
    "                scores = F[i-1] + self.pairwise_weights[:, y] + np.dot(X[:, i].T, self.weights[:, y]) + self.biases[y]\n",
    "                F[i, y] = np.max(scores)\n",
    "                Y[i, y] = np.argmax(scores)\n",
    "\n",
    "        y_pred = np.zeros(L, dtype=int)\n",
    "        y_pred[-1] = np.argmax(F[-1])\n",
    "        for i in range(L - 2, -1, -1):\n",
    "            y_pred[i] = Y[i + 1, y_pred[i + 1]]\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def train(self, X, Y, epochs=30):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                y_pred_indices = self.predict_sequence(X[i])\n",
    "                y_true_indices = [self.char_to_index(char) for char in Y[i]]\n",
    "\n",
    "                for j in range(X[i].shape[1]):\n",
    "                    x = X[i][:, j]\n",
    "                    y_pred = y_pred_indices[j]\n",
    "                    y_true = y_true_indices[j]\n",
    "\n",
    "                    # Hinge loss update\n",
    "                    if y_pred != y_true:\n",
    "                        self.weights[:, y_pred] -= self.C * x\n",
    "                        self.weights[:, y_true] += self.C * x\n",
    "                        self.biases[y_pred] -= self.C\n",
    "                        self.biases[y_true] += self.C\n",
    "\n",
    "                        if j > 0:\n",
    "                            y_pred_prev = y_pred_indices[j-1]\n",
    "                            y_true_prev = y_true_indices[j-1]\n",
    "                            self.pairwise_weights[y_true_prev, y_true] += self.C\n",
    "                            self.pairwise_weights[y_pred_prev, y_pred] -= self.C\n",
    "\n",
    "    def generate_predictions(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for sequence in X:\n",
    "            y_pred = self.predict_sequence(sequence)\n",
    "            predictions.append([self.index_to_char(index) for index in y_pred])\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate_predictions(self, predictions, Y):\n",
    "        total_chars = 0\n",
    "        correct_predictions = 0\n",
    "        correct_sequences = 0\n",
    "\n",
    "        for i in range(len(predictions)):\n",
    "            sequence_correct = True\n",
    "            for j in range(len(predictions[i])):\n",
    "                y_pred = predictions[i][j]\n",
    "                y_true = Y[i][j]\n",
    "\n",
    "                if y_pred == y_true:\n",
    "                    correct_predictions += 1\n",
    "                else:\n",
    "                    sequence_correct = False\n",
    "                total_chars += 1\n",
    "\n",
    "            if sequence_correct:\n",
    "                correct_sequences += 1\n",
    "\n",
    "        R_char = 1 - (correct_predictions / total_chars)\n",
    "        R_seq = 1 - (correct_sequences / len(Y))\n",
    "        return R_char, R_seq\n",
    "\n",
    "structured_svm = StructuredSVM(n_features, n_classes)\n",
    "structured_svm.train(trn_X, trn_Y)\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "predictions = structured_svm.generate_predictions(tst_X)\n",
    "\n",
    "# Evaluate the predictions\n",
    "R_char, R_seq = structured_svm.evaluate_predictions(predictions, tst_Y)\n",
    "print(\"Character Prediction Error (R_char):\", R_char)\n",
    "print(\"Sequence Prediction Error (R_seq):\", R_seq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentary:**\n",
    "I have attempted to modify my classifier from second assignment to align with fifth assignment. In order to do that, I have added regularisation parameter, characteristic for SVMs, used to balance the margin maximization and loss minimization. Furthermore, I have and modified train method by adding hinge loss update to it, in order to implement some of SVM principles, such as margin maximization and penalization of misclassifications, to the classificator.\n",
    "\n",
    "Other than that, the classifier remains structured output classifier with pairwise dependency.\n",
    "\n",
    "As for the results, the classifier trained using SVM based algorithm shows substantialy better results than it's previous version with almost halved sequence prediction error. I find it strange, since it uses R<sub>char</sub> as target loss function. R<sub>char</sub> is slightly better, but not by as large margin as R<sub>seq</sub>. \n",
    "\n",
    "The classifier could be further optimised by choosing different regularisation parameter.\n",
    "\n",
    "I am not sure if this implementation fully alignts with assignment 5, but I am pretty tired at this point, and at least I tried."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
